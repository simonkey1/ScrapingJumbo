{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.jumbo.cl/despensa/arroz-y-legumbres'\n",
    "\n",
    "# Realizamos la solicitud HTTP para obtener el HTML\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# Creamos el objeto BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "productos = soup.find_all('div', class_='product-card')\n",
    "\n",
    "productos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testear si se obtiene las url de los productos\n",
    "enlaces_productos = []\n",
    "for producto in productos:\n",
    "    # Buscar el anchor con la clase específica dentro de cada producto\n",
    "    anchor = producto.find('a', class_='product-card-image-link not-logged')\n",
    "    if anchor:\n",
    "        enlace = anchor['href']  # Extraer el link del anchor\n",
    "        enlaces_productos.append(enlace)\n",
    "\n",
    "# 4. Mostrar o procesar los enlaces\n",
    "for enlace in enlaces_productos:\n",
    "    print(enlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "base_url = 'https://www.jumbo.cl/despensa/arroz-y-legumbres?page='  # Asegúrate de que la URL base sea correcta\n",
    "pagina = 1\n",
    "enlaces_productos = []\n",
    "\n",
    "while True:  # Terminar en la página 13\n",
    "    # Concatenar el número de página a la URL base\n",
    "    url = f'{base_url}{pagina}'\n",
    "    print(f'Accediendo a: {url}')\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Buscar los divs que contienen los productos\n",
    "    productos = soup.find_all('div', class_='product-card')  # Ajustar la clase si es necesario\n",
    "    \n",
    "    # Si no se encuentran productos, romper el loop\n",
    "    if not productos:\n",
    "        print(f\"No se encontraron productos en la página {pagina}. Terminando paginación.\")\n",
    "        break\n",
    "\n",
    "    # Extraer los anchors dentro de cada tarjeta de producto\n",
    "    for producto in productos:\n",
    "        anchor = producto.find('a', class_='product-card-image-link not-logged')\n",
    "        if anchor:\n",
    "            enlace = anchor['href']\n",
    "            enlaces_productos.append(enlace)\n",
    "    \n",
    "    # Ir a la siguiente página\n",
    "    pagina += 1\n",
    "\n",
    "# Mostrar o procesar los enlaces obtenidos\n",
    "for enlace in enlaces_productos:\n",
    "    print(enlace)\n",
    "\n",
    "# Guardar los enlaces en un archivo CSV\n",
    "with open('enlaces_legumbres_arroz.csv', mode='w', newline='') as archivo_csv:\n",
    "    writer = csv.writer(archivo_csv)\n",
    "    writer.writerow(['enlace'])  # Cabecera del archivo CSV\n",
    "    for enlace in enlaces_productos:\n",
    "        writer.writerow([enlace])\n",
    "\n",
    "\n",
    "print(\"Enlaces guardados en 'enlaces_pastas_salsas.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'enlaces_legumbres_arroz.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Leer el archivo CSV con los enlaces\u001b[39;00m\n\u001b[0;32m      4\u001b[0m enlaces \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menlaces_legumbres_arroz.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m archivo_csv:\n\u001b[0;32m      6\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(archivo_csv)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mnext\u001b[39m(reader)  \u001b[38;5;66;03m# Saltar la cabecera\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SIMON\\Desktop\\Kaggle\\scraping\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'enlaces_legumbres_arroz.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "base_dir = Path(\"c:/Users/SIMON/Desktop/Kaggle/scraping\")\n",
    "ruta = base_dir / \"enlaces_productos\" / 'enlaces_pastas_salsas.csv'\n",
    "\n",
    "# Leer el archivo CSV con los enlaces\n",
    "enlaces = []\n",
    "with open('enlaces_legumbres_arroz.csv', mode='r') as archivo_csv:\n",
    "    reader = csv.reader(archivo_csv)\n",
    "    next(reader)  # Saltar la cabecera\n",
    "    for row in reader:\n",
    "        enlaces.append(row[0])  # Cada enlace es una fila\n",
    "\n",
    "print(\"Enlaces leídos:\", len(enlaces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "# Configuración de las opciones de Chrome\n",
    "options = Options()\n",
    "options.binary_location = r'C:\\Program Files\\BraveSoftware\\Brave-Browser\\Application\\brave.exe'\n",
    "service = Service(r\"C:\\Users\\SIMON\\Desktop\\Kaggle\\scraping\\chromedriver.exe\")\n",
    "\n",
    "# Inicializa el driver\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Leer enlaces desde el archivo CSV, ignorando el carácter ';'\n",
    "with open('enlaces_legumbres_arroz.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file, delimiter=',')  # Usa la coma como delimitador\n",
    "    enlaces = [row[0].strip().replace(';', '') for row in reader if row]  # Limpiar espacios y eliminar ';'\n",
    "\n",
    "# Limitar a los primeros 10 enlaces\n",
    "enlaces = enlaces[:300]\n",
    "\n",
    "# Lista para almacenar los datos nutricionales\n",
    "datos_nutricionales = []\n",
    "\n",
    "# Procesar cada enlace\n",
    "for enlace in enlaces:\n",
    "    url_producto = f'https://www.jumbo.cl{enlace}'  # Construir la URL del producto\n",
    "    print(f'Accediendo a: {url_producto}')\n",
    "    \n",
    "    # Crear un diccionario para almacenar los nutrientes del producto\n",
    "    producto_info = {\n",
    "        'Marca': '',\n",
    "        'Codigo Producto': '',\n",
    "        'Nombre Producto': '',\n",
    "        'Precio': '',\n",
    "        'Nutrientes': [],  # Lista para almacenar los nutrientes\n",
    "        'Ingredientes': ''  # Campo para almacenar los ingredientes\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        driver.get(url_producto)\n",
    "        time.sleep(2)  # Espera a que la página cargue\n",
    "\n",
    "        # Extraer el H1\n",
    "        h1_element = driver.find_element(By.CSS_SELECTOR, 'h1.product-name.text-black.text-lg.font-bold.lg\\\\:text-2xl.lg\\\\:mb-0\\\\.5')\n",
    "        marca = driver.find_element(By.CSS_SELECTOR, 'span.product-brand.leading-\\\\[18px\\\\].text-primary500.text-base.font-semibold.text-black-600.underline.capitalize')\n",
    "        marca_producto = marca.text.strip()\n",
    "        nombre_producto = h1_element.text.strip()\n",
    "        id_product = driver.find_element(By.CSS_SELECTOR, 'span.product-code.text-greyMidDark.text-sm.lg\\\\:mb-2')\n",
    "        codigo_producto = id_product.text.strip()  # Obtener el id\n",
    "        precio = driver.find_element(By.CSS_SELECTOR, 'span.prices-main-price')\n",
    "        precio = precio.text.strip()  # Obtener el precio del producto\n",
    "        # Hacer clic en la pestaña de \"Información nutricional\"\n",
    "        informacion_nutricional_tab = driver.find_element(By.XPATH, \"//span[contains(text(), 'Información nutricional')]\")\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", informacion_nutricional_tab)\n",
    "        informacion_nutricional_tab.click()\n",
    "        \n",
    "        \n",
    "\n",
    "        time.sleep(2)  # Espera a que se cargue la tabla\n",
    "\n",
    "        # Encuentra todos los <li> dentro de los <ul> que tienen la clase 'nutritional-details-container-data'\n",
    "        li_elements = driver.find_elements(By.CSS_SELECTOR, 'ul.nutritional-details-container-data li')\n",
    "\n",
    "        # Verificar cuántos <li> se encontraron\n",
    "        print(f'Total de <li> encontrados: {len(li_elements)}')\n",
    "\n",
    "        # Actualizar el diccionario con la información del producto\n",
    "        producto_info['Marca'] = marca_producto\n",
    "        producto_info['Codigo Producto'] = codigo_producto\n",
    "        producto_info['Nombre Producto'] = nombre_producto\n",
    "        producto_info['Precio'] = precio\n",
    "\n",
    "        # Procesar los elementos, asumiendo que la estructura es nutriente seguido de dos valores\n",
    "        for i in range(0, len(li_elements), 3):  # Incrementar de 3 en 3\n",
    "            if i + 2 < len(li_elements):  # Asegurarse de que hay suficiente para evitar errores de índice\n",
    "                nutriente = li_elements[i].text.strip()  # Nombre del nutriente\n",
    "                valor_por_100g = li_elements[i + 1].text.strip()  # Valor por cada 100g\n",
    "                valor_por_porcion = li_elements[i + 2].text.strip()  # Valor por porción\n",
    "                print(f'Nutriente: {nutriente}, Valor por 100g: {valor_por_100g}, Valor por porción: {valor_por_porcion}')\n",
    "                \n",
    "                # Añadir el nutriente a la lista dentro del diccionario del producto\n",
    "                producto_info['Nutrientes'].append({\n",
    "                    'Nutriente': nutriente,\n",
    "                    'Valor por 100g': valor_por_100g,\n",
    "                    'Valor por porcion': valor_por_porcion\n",
    "                })\n",
    "            else:\n",
    "                print(f'Elemento sin suficientes valores encontrado: {li_elements[i].text.strip()}')\n",
    "\n",
    "        # Añadir el diccionario del producto a la lista de datos\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar {url_producto}: {str(e)}\")\n",
    "\n",
    "    try:\n",
    "        ingredientes_tab = driver.find_element(By.XPATH, \"//span[contains(text(), 'Ingredientes')]\")\n",
    "        ingredientes_tab.click()\n",
    "        time.sleep(1)  # Espera breve para asegurar que el contenido se carga\n",
    "\n",
    "        # Extrae el contenido del div correspondiente a los ingredientes\n",
    "        ingredientes_div = driver.find_element(By.CSS_SELECTOR, 'div.text-base.leading-5')\n",
    "        ingredientes_texto = ingredientes_div.text.strip()\n",
    "\n",
    "        # Agrega los ingredientes al diccionario del producto\n",
    "        producto_info['Ingredientes'] = ingredientes_texto\n",
    "        print(f'Ingredientes: {ingredientes_texto}')\n",
    "\n",
    "    except:\n",
    "        # Si no se encuentra la pestaña \"Ingredientes\", simplemente continúas sin agregarla\n",
    "        print(\"No se encontró la pestaña de 'Ingredientes' para este producto.\")\n",
    "    datos_nutricionales.append(producto_info)\n",
    "# Comprobar si se capturaron datos antes de guardar\n",
    "if datos_nutricionales:\n",
    "    # Guardar los datos en un archivo JSON\n",
    "    nombre_archivo_json = 'datos_pastas_y_salsas.json'\n",
    "    with open(nombre_archivo_json, 'w', encoding='utf-8') as file:\n",
    "        json.dump(datos_nutricionales, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Datos guardados en {nombre_archivo_json}\")\n",
    "\n",
    "    # También puedes guardar en CSV si lo deseas\n",
    "    # Ten en cuenta que el formato del CSV será diferente debido a la estructura\n",
    "    nombre_archivo_csv = 'datos_legumbres_y_arroz.csv'\n",
    "    with open(nombre_archivo_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Codigo Producto', 'Nombre Producto', 'Nutriente', 'Valor por 100g', 'Valor por porcion']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        for producto in datos_nutricionales:\n",
    "            for nutriente in producto['Nutrientes']:\n",
    "                writer.writerow({\n",
    "                    'Codigo Producto': producto['Codigo Producto'],\n",
    "                    'Nombre Producto': producto['Nombre Producto'],\n",
    "                    'Nutriente': nutriente['Nutriente'],\n",
    "                    'Valor por 100g': nutriente['Valor por 100g'],\n",
    "                    'Valor por porcion': nutriente['Valor por porcion']\n",
    "                })\n",
    "\n",
    "    print(f\"Datos guardados en {nombre_archivo_csv}\")\n",
    "else:\n",
    "    print(\"No se encontraron datos nutricionales.\")\n",
    "\n",
    "# Cerrar el driver después de terminar\n",
    "driver.quit()\n",
    "\n",
    "# Cargar y mostrar el DataFrame\n",
    "df = pd.read_json('datos_legumbres_y_arroz.json')\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('datos_pastas_y_salsas.json')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Cargar el JSON en un DataFrame\n",
    "with open('datos_pastas_y_salsas.json', 'r', encoding='utf-8') as file:\n",
    "    datos = json.load(file)\n",
    "\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.json_normalize(datos, 'Nutrientes', ['Marca','Codigo Producto','Nombre Producto', 'Precio', 'Ingredientes'], errors='ignore')\n",
    "\n",
    "# Verificar si las columnas 'Valor por 100g' y 'Valor por porción' existen en el DataFrame\n",
    "    # Pivotar el DataFrame para obtener formato ancho\n",
    "df_ancho4 = df.pivot_table(index=['Nombre Producto', 'Precio', 'Codigo Producto', 'Marca', 'Ingredientes'], \n",
    "                               columns='Nutriente', \n",
    "                               values=['Valor por 100g', 'Valor por porcion'], \n",
    "                               aggfunc='first')\n",
    "\n",
    "# Aplanar las columnas multiíndice\n",
    "df_ancho4.columns = [f'{nutriente} {valor}' for valor, nutriente in df_ancho4.columns]\n",
    "\n",
    "# Resetear el índice\n",
    "df_ancho4.reset_index(inplace=True)\n",
    "\n",
    "# Mostrar el DataFrame final\n",
    "\n",
    "\n",
    "df_ancho4.columns = df_ancho4.columns.str.strip().str.lower() \n",
    "df_ancho4.columns = df_ancho4.columns.str.replace(' ', '_')\n",
    "\n",
    "\n",
    "# Diccionario de mapeo para abreviaturas en inglés\n",
    "abbreviations = {\n",
    "    'azúcares_totales_(g)_valor_por_100g': 'total_sugars_100g',\n",
    "    'colesterol_(mg)_valor_por_100g': 'cholesterol_100g',\n",
    "    'energía_(kcal)_valor_por_100g': 'energy_100g',\n",
    "    'fibra_(g)_valor_por_100g': 'fiber_100g',\n",
    "    'grasas_monoinsaturadas_(g)_valor_por_100g': 'mono_fats_100g',\n",
    "    'grasas_poliinsaturadas_(g)_valor_por_100g': 'poly_fats_100g',\n",
    "    'fibra_(g)_valor_por_porción': 'fiber_serving',\n",
    "    'grasas_monoinsaturadas_(g)_valor_por_porción': 'mono_fats_serving',\n",
    "    'grasas_poliinsaturadas_(g)_valor_por_porción': 'poly_fats_serving',\n",
    "    'grasas_saturadas_(g)_valor_por_porción': 'sat_fats_serving',\n",
    "    'grasas_totales_(g)_valor_por_porción': 'total_fats_serving',\n",
    "    'grasas_trans_(g)_valor_por_porción': 'trans_fats_serving',\n",
    "    'hidratos_de_carbono_disponibles_(g)_valor_por_porción': 'available_carbs_serving',\n",
    "    'proteínas_(g)_valor_por_porción': 'proteins_serving',\n",
    "    'sodio_(mg)_valor_por_porción': 'sodium_serving',\n",
    "}\n",
    "\n",
    "\n",
    "df_ancho4.rename(columns=abbreviations, inplace=True)\n",
    "\n",
    "df_ancho4['tamaño'] = df_ancho4['nombre_producto'].str.extract(r'(\\d+(?:\\.\\d+)?\\s*[a-zA-Z]+)$')\n",
    "\n",
    "\n",
    "def separate_size(size):\n",
    "    match = re.match(r'(\\d+(?:\\.\\d+)?)\\s*(L|ml|g|kg|cc)', size)\n",
    "    if match:\n",
    "        value, unit = match.groups()\n",
    "        return pd.Series([value, unit])  # Devolver como serie\n",
    "    return pd.Series([None, None])  # Si no hay coincidencia\n",
    "\n",
    "# Convert 'tamaño' column to string type\n",
    "df_ancho4['tamaño'] = df_ancho4['tamaño'].astype(str)\n",
    "\n",
    "# Aplicar la función y crear nuevas columnas para Valor y Unidad\n",
    "df_ancho4[['valor', 'unidad']] = df_ancho4['tamaño'].apply(separate_size)\n",
    "\n",
    "df_ancho4.drop(columns=['tamaño'], inplace=True)\n",
    "\n",
    "df_ancho4['marca'] = df_ancho4['marca'].replace('Frutas Y Verduras Propias', 'Jumbo')\n",
    "df_ancho4['precio'] = df_ancho4['precio'].str.replace('$', '', regex=False)\n",
    "df_ancho4['precio'] = df_ancho4['precio'].str.replace('.', '', regex=False)\n",
    "df_ancho4['precio'] = df_ancho4['precio'].replace('', '0').astype(int)  # Replace empty strings with '0' before converting to int\n",
    "df_ancho4['codigo_producto'] = df_ancho4['codigo_producto'].str.replace('Código: ', '', regex=False)\n",
    "\n",
    "df_ancho4.fillna(0, inplace=True)\n",
    "\n",
    "cols_to_convert = [\n",
    "    \"total_sugars_100g\", \"cholesterol_100g\", \"energy_100g\", \"fiber_100g\",\n",
    "    \"mono_fats_100g\", \"poly_fats_100g\", \"grasas_saturadas_(g)_valor_por_100g\",\n",
    "    \"grasas_totales_(g)_valor_por_100g\", \"grasas_trans_(g)_valor_por_100g\",\n",
    "    \"hidratos_de_carbono_disponibles_(g)_valor_por_100g\", \"proteínas_(g)_valor_por_100g\",\n",
    "    \"sodio_(mg)_valor_por_100g\", \"azúcares_totales_(g)_valor_por_porción\",\n",
    "    \"colesterol_(mg)_valor_por_porción\", \"energía_(kcal)_valor_por_porción\",\n",
    "    \"fiber_serving\", \"mono_fats_serving\", \"poly_fats_serving\", \"sat_fats_serving\",\n",
    "    \"total_fats_serving\", \"trans_fats_serving\", \"available_carbs_serving\",\n",
    "    \"proteins_serving\", \"sodium_serving\"\n",
    "]\n",
    "\n",
    "# Convertir cada columna a tipo float\n",
    "# Verificar las columnas existentes en df_ancho2\n",
    "existing_columns = df_ancho4.columns\n",
    "\n",
    "# Filtrar cols_to_convert para incluir solo las columnas que existen en df_ancho2\n",
    "cols_to_convert = [col for col in cols_to_convert if col in existing_columns]\n",
    "\n",
    "for col in cols_to_convert:\n",
    "    df_ancho4[col] = df_ancho4[col].astype(str).str.replace(r'[^\\d]', '', regex=True)  \n",
    "    df_ancho4[col] = pd.to_numeric(df_ancho4[col])  # Convierte a float\n",
    "\n",
    "# Confirmar las conversiones\n",
    "\n",
    "\n",
    "\n",
    "df_ancho4.fillna(0, inplace=True)\n",
    "\n",
    "df_ancho4['tipo_de_producto'] = 'Pastas y Salsas'\n",
    "\n",
    "df_ancho4.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
